---
title: Microsoft Translator Speech API リファレンス | Microsoft Docs
titleSuffix: Cognitive Services
description: Microsoft Translator Speech API のリファレンス ドキュメント。
services: cognitive-services
author: Jann-Skotdal
manager: chriswendt1
ms.service: cognitive-services
ms.technology: microsoft translator
ms.topic: article
ms.date: 05/18/2018
ms.author: v-jansko
ms.openlocfilehash: be8faddf56158de3399713c41638c0b913b4627e
ms.sourcegitcommit: 95d9a6acf29405a533db943b1688612980374272
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 06/23/2018
ms.locfileid: "35378403"
---
# <a name="microsoft-translator-speech-api"></a>Microsoft Translator Speech API

このサービスには、ある言語の会話音声を別の言語のテキストに書き起こすストリーミング API が用意されています。 この API では、翻訳済みテキストを読み上げるテキスト読み上げ機能も統合されています。 Microsoft Translator Speech API では、Skype 翻訳で提供されている会話のリアルタイムの翻訳のようなシナリオを実現できます。

Microsoft Translator Speech API を使用することにより、クライアント アプリケーションは音声オーディオをサービスにストリーミングし、テキスト ベースの結果のストリームを受け取ります。このストリームには、ソース言語で認識されたテキストとターゲット言語でのその翻訳が含まれます。 テキストの結果は、ディープ ニューラル ネットワークで強化された自動音声認識 (ASR) を、受信するオーディオ ストリームに適用することによって生成されます。 生の ASR 出力は、ユーザーの意図をより厳密に反映するために、TrueText と呼ばれる新しい手法でさらに改善されます。 たとえば、TrueText では吃り (うーんといった声や咳) を削除し、適切な区切り記号と大文字/小文字の設定を復元します。 不適切な表現をマスクまたは除外する機能も含まれています。 認識エンジンおよび翻訳エンジンは、会話音声を処理するように特にトレーニングされます。 音声翻訳サービスでは、無音検出を使用して発話の終わりを特定します。 音声アクティビティがしばらく行われないと、サービスは完了した発話の最終的な結果をストリーミングして返します。 サービスはまた、部分的な結果を返送することもできます。この結果には進行中の発話の中間的な認識および翻訳が含まれます。 最終的な結果として、サービスではターゲット言語での音声テキストからの音声を合成する機能 (テキスト読み上げ) を提供します。 テキスト読み上げオーディオは、クライアントが指定した形式で作成されます。 WAV および MP3 形式を使用できます。

Microsoft Translator Speech API では、WebSocket プロトコルを活用して、クライアントとサーバーの間に全二重通信チャネルを提供します。 アプリケーションでこのサービスを使用するには次の手順が必要です。

## <a name="1-getting-started"></a>1.使用の開始
Microsoft Translator Text API にアクセスするには、[Microsoft Azure](translator-speech-how-to-signup.md) にサインアップする必要があります。

## <a name="2-authentication"></a>2.認証

サブスクリプション キーを使用して認証を行います。 Microsoft Translator Speech API では、次の 2 つの認証モードがサポートされています。

* **アクセス トークンを使用する:** アプリケーションでは、トークン サービスからアクセス トークンを取得します。 Microsoft Translator Speech API サブスクリプション キーを使用して、Cognitive Services 認証サービスからアクセス トークンを取得します。 アクセス トークンは 10 分間有効です。 10 分ごとに新しいアクセス トークンを取得し、このような 10 分以内で繰り返す要求に対して同じアクセス トークンを使い続けます。

* **サブスクリプション キーを直接使用する:** アプリケーション内で、サブスクリプション キーを `Ocp-Apim-Subscription-Key` ヘッダーの値として渡します。

サブスクリプション キーとアクセス トークンは、非表示とする必要があるシークレットとして扱います。

## <a name="3-query-languages"></a>手順 3.クエリ言語
**現在のサポートされている言語のセット用の言語リソースのクエリを実行します。** [言語リソース](languages-reference.md)では、音声認識、テキスト翻訳、およびテキスト読み上げで利用できる言語および音声のセットを公開します。 各言語または音声には識別子が与えられ、Microsoft Translator Speech API はその識別子を使用して同じ言語または音声を識別します。

## <a name="4-stream-audio"></a>4.オーディオのストリーミング
**接続を開き、サービスへのオーディオのストリーミングを開始します。** サービス URL は `wss://dev.microsofttranslator.com/speech/translate` です。 サービスによって期待されるパラメーターおよびオーディオの形式については、後述する `/speech/translate` 操作を参照してください。 パラメーターのいずれかを使用して、上記の手順 2 で取得したアクセス トークンが渡されます。

## <a name="5-process-the-results"></a>5.結果の処理
**サービスからストリーミングされ返された結果を処理します。** 部分的な結果、最終的な結果、およびテキスト読み上げオーディオ セグメントの形式については、後述する `/speech/translate` 操作を参照してください。

Microsoft Translator Speech API の使用例を示すコード サンプルは、[Microsoft Translator Github サイト](https://github.com/MicrosoftTranslator)から入手できます。

## <a name="implementation-notes"></a>実装に関するメモ

GET /speech/translate によって音声翻訳のセッションが確立されます

### <a name="connecting"></a>接続
サービスに接続する前に、このセクションで後述するパラメーターの一覧を確認してください。 要求例を次に示します。

`GET wss://dev.microsofttranslator.com/speech/translate?from=en-US&to=it-IT&features=texttospeech&voice=it-IT-Elsa&api-version=1.0`
`Ocp-Apim-Subscription-Key: {subscription key}`
`X-ClientTraceId: {GUID}`

この要求では、英語による会話がサービスにストリーミングされて、イタリア語に翻訳されるように指定しています。 それぞれの最終的な認識結果では、Elsa という名前の女性の声でテキスト読み上げオーディオ応答が生成されます。 要求では `Ocp-Apim-Subscription-Key header` に資格情報が含まれていることに注目してください。 また、要求ではベスト プラクティスに従うために、ヘッダー `X-ClientTraceId` にグローバル一意識別子を設定します。 クライアント アプリケーションでは、問題が発生したときにトラブルシューティングに使用できるようにトレース ID を記録する必要があります。

### <a name="sending-audio"></a>オーディオの送信
接続が確立されると、クライアントはサービスへのオーディオのストリーミングを開始します。 クライアントは、オーディオをチャンク単位で送信します。 各チャンクは Binary 型の Websocket メッセージを使用して送信されます。

オーディオ入力は Waveform オーディオ ファイル形式 (WAVE またはファイル名の拡張子からよく知られている WAV) となります。 クライアント アプリケーションは、16 KHz でサンプリングされる、単一チャネルの符号付き 16 ビット PCM オーディオをストリーミングする必要があります。 クライアントによってストリーミングされる最初のバイト セットには、WAV ヘッダーが含められます。 16 KHz でサンプリングされる、単一チャネルの符号付き 16 ビット PCM ストリームに対する 44 バイト ヘッダーは次のとおりです。

|Offset|値|
|:---|:---|
|0 - 3|"RIFF"|
|4 - 7|0|
|8 - 11|"WAVE"|
|12 - 15|"fmt"|
|16 - 19|16|
|20 - 21|1|
|22 - 23|1|
|24 - 27|16000|
|28 - 31|32000|
|32 - 33|2|
|34 - 35|16|
|36 - 39|"data"|
|40 - 43|0|

ファイル サイズの合計 (バイト 4 から 7) および "data" のサイズ (バイト 40 から 43) が 0 に設定されていることに注目してください。 合計サイズがあらかじめわかっているとは限らないストリーミング シナリオの場合、これは問題ありません。

クライアントは、WAV (RIFF) ヘッダーの送信後にオーディオ データのチャンクを送信します。 クライアントは通常、固定期間を表す固定サイズのチャンクをストリーミングします (たとえば、一度に 100 ms のオーディオをストリーミングします)。

### <a name="final-result"></a>最終的な結果
最終的な音声認識の結果は、発話の最後で生成されます。 Text 型の WebSocket メッセージを使用してサービスからクライアントに結果が送信されます。 メッセージの内容は、次のプロパティを持つオブジェクトの JSON のシリアル化となります。

* `type`: 結果の型を識別する文字列定数。 この値は最終的な結果を表す final です。
* `id`: 認識結果に割り当てられる文字列識別子。
* `recognition`: ソース言語で認識されたテキスト。 認識に失敗した場合、テキストは空の文字列となる場合があります。
* `translation`: ターゲット言語で翻訳される認識されたテキスト。
* `audioTimeOffset`: ティック単位での認識開始の時間オフセット (1 ティック = 100 ナノ秒)。 オフセットはストリーミングの開始を基準とします。
* `audioTimeSize`: ティック (100 ナノ秒) 単位での認識の期間。
* `audioStreamPosition`: 認識開始のバイト オフセット。 オフセットはストリームの先頭を基準とします。
* `audioSizeBytes`: バイト単位の認識サイズ。

既定ではオーディオ ストリーム内の認識の位置が結果に含まれていないことに注目してください。 `TimingInfo` 機能はクライアントによって選択される必要があります (`features` パラメーターを参照)。

サンプルの最終的な結果は次のとおりです。

```
{
  type: "final"
  id: "23",
  recognition: "what was said", 
  translation: "translation of what was said",
  audioStreamPosition: 319680,
  audioSizeBytes: 35840,
  audioTimeOffset: 2731600000,
  audioTimeSize: 21900000
}
```

### <a name="partial-result"></a>部分的な結果
既定では、音声認識の部分的または中間的な結果はクライアントにストリーミングされません。 クライアントは features クエリ パラメーターを使用してそれらを要求することができます。

Text 型の WebSocket メッセージを使用してサービスからクライアントに部分的な結果が送信されます。 メッセージの内容は、次のプロパティを持つオブジェクトの JSON のシリアル化となります。

* `type`: 結果の型を識別する文字列定数。 この値は部分的な結果を表す partial です。
* `id`: 認識結果に割り当てられる文字列識別子。
* `recognition`: ソース言語で認識されたテキスト。
* `translation`: ターゲット言語で翻訳される認識されたテキスト。
* `audioTimeOffset`: ティック単位での認識開始の時間オフセット (1 ティック = 100 ナノ秒)。 オフセットはストリーミングの開始を基準とします。
* `audioTimeSize`: ティック (100 ナノ秒) 単位での認識の期間。
* `audioStreamPosition`: 認識開始のバイト オフセット。 オフセットはストリームの先頭を基準とします。
* `audioSizeBytes`: バイト単位の認識サイズ。

既定ではオーディオ ストリーム内の認識の位置が結果に含まれていないことに注目してください。 TimingInfo 機能はクライアントによって選択される必要があります (features パラメーターを参照)。

サンプルの最終的な結果は次のとおりです。

```
{
  type: "partial"
  id: "23.2",
  recognition: "what was", 
  translation: "translation of what was",
  audioStreamPosition: 319680,
  audioSizeBytes: 25840,
  audioTimeOffset: 2731600000,
  audioTimeSize: 11900000
}
```

### <a name="text-to-speech"></a>テキスト読み上げ
テキスト読み上げ機能が有効である場合 (下の `features` パラメーターを参照)、最終的な結果の後に翻訳されたテキスト読み上げのオーディオが続きます。 オーディオ データはチャンクされて、Binary 型の Websocket メッセージのシーケンスとしてサービスからクライアントに送信されます。 クライアントは、各メッセージの FIN ビットを確認することにより、ストリームの終わりを検出できます。 最後の Binary メッセージの FIN ビットは 1 に設定されて、ストリームの終わりを示します。 ストリームの形式は `format` パラメーターの値によって決まります。

### <a name="closing-the-connection"></a>接続を閉じる
クライアント アプリケーションは、オーディオのストリーミングを終了し、最後の最終的な結果を受信したら、WebSocket 終了ハンドシェイクを開始することによって接続を閉じる必要があります。 サーバーが接続を終了するには、条件があります。 クライアントは次の WebSocket Closed コードを受信する可能性があります。

* `1003 - Invalid Message Type`: サーバーは、受信したデータ型を受け入れられないため、接続を終了します。 これは一般に、受信オーディオが適切なヘッダーで開始されていない場合に発生します。
* `1000 - Normal closure`: 要求が満たされた後、接続は閉じられています。 サーバーが接続を閉じるのは、長期間にわたってクライアントからオーディオが受信されなかった場合、長期間にわたって無音声がストリーミングされた場合、セッションが最大有効期間 (約 90 分) に達した場合です。
* `1001 - Endpoint Unavailable`: サーバーが利用できなくなることを示します。 クライアント アプリケーションは再接続を試みる場合があります。再試行の回数には制限があります。
* `1011 - Internal Server Error`: サーバー上でエラーが発生したために、サーバーによって接続が閉じられます。

### <a name="parameters"></a>parameters

|パラメーター|値|説明|パラメーターのタイプ|データ型|
|:---|:---|:---|:---|:---|
|api-version|1.0|クライアントによって要求される API のバージョン。 使用できる値: `1.0`。|クエリ   |文字列|
|from|(空)   |受信する音声の言語を指定します。 Languages API からの応答内の `speech` スコープからの言語識別子のいずれかが値となります。|クエリ|文字列|
|to|(空)|書き起こされたテキストの翻訳先言語を指定します。 Languages API からの応答内の `text` スコープからの言語識別子のいずれかが値となります。|クエリ|文字列|
|features|(空)   |クライアントによって選択された機能セットであり、コンマで区切られます。 使用できる機能は次のとおりです。<ul><li>`TextToSpeech`: 最終的に翻訳された文のオーディオがサービスから返される必要があることを指定します。</li><li>`Partial`: オーディオがサービスにストリーミングされている間に、中間的な認識結果がサービスから返される必要があることを指定します。</li><li>`TimingInfo`: 各認識に関連付けられたタイミング情報がサービスから返される必要があることを指定します。</li></ul>たとえば、クライアントが `features=partial,texttospeech` を指定した場合、部分的な結果とテキスト読み上げは返されますが、タイミング情報は返されません。 最終結果は常に、クライアントにストリーミングされます。|クエリ|文字列|
|voice|(空)|翻訳済みテキストのテキスト読み上げレンダリングに使用する音声を識別します。 Languages API からの応答内の tts スコープからの言語識別子のいずれかが、値となります。 音声が指定されていない場合は、テキスト読み上げが有効にされると、システムは自動的に音声を選択します。|クエリ|文字列|
|format|(空)|サービスによって返されるテキスト読み上げオーディオ ストリームの形式を指定します。 使用できるオプションは次のとおりです。<ul><li>`audio/wav`: Waveform オーディオ ストリーム。 クライアントは WAV ヘッダーを使用することで、オーディオ形式を正しく解釈する必要があります。 テキスト読み上げ用の WAV オーディオは、24 KHz または 16 KHz でサンプリングされる、単一チャネルの 16 ビット PCM です。</li><li>`audio/mp3`: MP3 オーディオ ストリーム。</li></ul>既定値は `audio/wav` です。|クエリ|文字列|
|ProfanityAction    |(空)    |音声で認識された不適切な表現をサービスで処理する方法を指定します。 有効なアクションは次のとおりです。<ul><li>`NoAction`: 不適切な表現は現状のままです。</li><li>`Marked`: 不適切な表現はマーカーに置き換えられます。 `ProfanityMarker` パラメーターを参照してください。</li><li>`Deleted`: 不適切な表現は削除されます。 たとえば、単語 `"jackass"` が不適切な表現として扱われた場合、フレーズ `"He is a jackass."` は `"He is a .".` になります。</li></ul>既定値は Marked です。|クエリ|文字列|
|ProfanityMarker|(空)    |`ProfanityAction` が `Marked` に設定されている場合に、検出された不適切な表現を処理する方法を指定します。 有効なオプションは次のとおりです。<ul><li>`Asterisk`: 不適切な表現は文字列 `***` に置き換えられます。 たとえば場合、単語 `"jackass"` が不適切な表現として扱われた場合、フレーズ `"He is a jackass."` は `"He is a ***.".` になります。</li><li>`Tag`: 不適切な表現は、不適切な表現の XML タグで囲まれます。 たとえば、単語 `"jackass"` が不適切な表現として扱われた場合、フレーズ `"He is a jackass."` は `"He is a <profanity>jackass</profanity>."` になります。</li></ul>既定では、 `Asterisk`です。|クエリ|文字列|
|Authorization|(空)  |クライアントのベアラー トークンの値を指定します。 プレフィックス `Bearer` の後に、認証トークン サービスから返された `access_token` 値の値を続けます。|ヘッダー   |文字列|
|Ocp-Apim-Subscription-Key|(空)|`Authorization` ヘッダーが指定されていない場合は必須。|ヘッダー|文字列|
|access_token|(空)   |有効な OAuth アクセス トークンを渡すための代替え方法です。 ベアラー トークンは通常、ヘッダー `Authorization` と共に提供されます。 Websocket ライブラリの中には、クライアント コードによるヘッダーの設定を許可していないものがあります。 このような場合、クライアントは `access_token` クエリ パラメーターを使用して有効なトークンを渡すことができます。 アクセス トークンを使用して認証するとき、`Authorization` ヘッダーが設定されていない場合は、`access_token` を設定する必要があります。 ヘッダーとクエリ パラメーターの両方が設定されている場合、クエリ パラメーターは無視されます。 クライアントは、トークンを渡すメソッドを 1 つ使用する必要があるだけです。|クエリ|文字列|
|subscription-key|(空)   |サブスクリプション キーを渡す代替手段。 Websocket ライブラリの中には、クライアント コードによるヘッダーの設定を許可していないものがあります。 このような場合、クライアントは `subscription-key` クエリ パラメーターを使用して有効なサブスクリプション キーを渡すことができます。 サブスクリプション キーを使用して認証するとき、`Ocp-Apim-Subscription-Key` ヘッダーが設定されていない場合は、サブスクリプション キーを設定する必要があります。 ヘッダーとクエリ パラメーターの両方が設定されている場合、クエリ パラメーターは無視されます。 クライアントは、`subscription key` を渡すメソッドを 1 つ使用する必要があるだけです。|クエリ|文字列|
|X-ClientTraceId    |(空)    |クライアントによって生成される GUID であり、要求を追跡するのに使用されます。 問題のトラブルシューティングを適切に行えるように、クライアントは要求のたびに新しい値を指定し、それを記録する必要があります。<br/>ヘッダーを使用するのでなく、この値をクエリ パラメーター `X-ClientTraceId` と共に渡すことができます。 ヘッダーとクエリ パラメーターの両方が設定されている場合、クエリ パラメーターは無視されます。|ヘッダー|文字列|
|X-CorrelationId|(空)    |クライアントによって生成される識別子であり、会話内で複数のチャネルを関連付けるのに使用されます。 複数の音声翻訳セッションを作成することで、ユーザー間の会話を実現することができます。 このようなシナリオでは、チャネルを結び付けるために、すべての音声翻訳セッションで同じ相関 ID が使用されます。 これにより、トレースと診断が容易になります。 識別子は `^[a-zA-Z0-9-_.]{1,64}$` に従う必要があります<br/>ヘッダーを使用するのでなく、この値をクエリ パラメーター `X-CorrelationId` と共に渡すことができます。 ヘッダーとクエリ パラメーターの両方が設定されている場合、クエリ パラメーターは無視されます。|ヘッダー|文字列|
|X-ClientVersion|(空)    |クライアント アプリケーションのバージョンを識別します。 例: "2.1.0.123"。<br/>ヘッダーを使用するのでなく、この値をクエリ パラメーター `X-ClientVersion` と共に渡すことができます。 ヘッダーとクエリ パラメーターの両方が設定されている場合、クエリ パラメーターは無視されます。|ヘッダー|文字列|
|X-OsPlatform|(空)   |クライアント アプリケーションが実行されているオペレーティング システムの名前とバージョンを識別します。 例: "Android 5.0"、"iOs 8.1.3"、"Windows 8.1"。<br/>ヘッダーを使用するのでなく、この値をクエリ パラメーター `X-OsPlatform` と共に渡すことができます。 ヘッダーとクエリ パラメーターの両方が設定されている場合、クエリ パラメーターは無視されます。|ヘッダー|文字列|

### <a name="response-messages"></a>応答メッセージ

|HTTP 状態コード|理由|応答モデル|headers|
|:--|:--|:--|:--|
|101    |WebSocket のアップグレード。|モデル値の例 <br/> オブジェクト {}|X-RequestId<br/>トラブルシューティングの目的で要求を識別する値。<br/>文字列|
|400    |要求が正しくありません。 入力パラメーターを調べて正しいことを確認します。 応答オブジェクトには、エラーに関する詳細な説明が含まれています。|||
|401    |権限がありません。 資格情報が設定されていること、資格情報が有効であること、および Azure Data Market サブスクリプションが健全な状態で、利用可能な残量があることを確認します。|||
|500    |エラーが発生しました。 エラーが解決しない場合は、それをクライアント トレース識別子 (X-ClientTraceId) または要求識別子 (X-RequestId) と一緒にご報告ください。|||
|503    |サーバーが一時的に使用できません。 要求を再試行してください。 エラーが解決しない場合は、それをクライアント トレース識別子 (X-ClientTraceId) または要求識別子 (X-RequestId) と一緒にご報告ください。|||

    


    





    
    




    




    




    

            




        









