---
title: 概念 | Microsoft Docs
description: Microsoft Speech Serviceで使用される基本概念について説明します。
services: cognitive-services
author: zhouwangzw
manager: wolfma
ms.service: cognitive-services
ms.component: bing-speech
ms.topic: article
ms.date: 09/15/2017
ms.author: zhouwang
ms.openlocfilehash: bc23f4fb7dfc045a0f8cc87155c31875c4de8450
ms.sourcegitcommit: 95d9a6acf29405a533db943b1688612980374272
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 06/23/2018
ms.locfileid: "35373973"
---
# <a name="basic-concepts"></a>基本的な概念

このページでは、Microsoft 音声認識サービスの基本的な概念について説明します。 アプリケーションで Microsoft 音声認識 API を使用する前に、このページを読むことをお勧めします。

## <a name="understanding-speech-recognition"></a>音声認識についての理解

音声対応のアプリケーションを初めて作成する場合や、既存のアプリケーションに音声認識機能を初めて追加する場合は、このセクションを読むことで作業をスムーズに開始できます。 既に音声対応アプリケーションの使用経験がある場合は、このセクションにはざっと目を通すだけでもかまいません。また、既に音声認識に熟達していて、早くプロトコルの詳細を知りたい場合は、このセクションを完全にスキップしてもかまいません。

### <a name="audio-streams"></a>音声ストリーム

音声認識の基本概念としてまず最初に理解するべきものは、*音声ストリーム*です。 ある一時点で発生し、1 つの情報だけを含むキーストロークとは異なり、口頭の要求は数百ミリ秒にわたって分散し、数キロバイトの情報を含みます。 音声の発話時間が長いと、開発者にとっては、アプリケーションを効率よくスムーズに動作させることが難しくなります。 今日のコンピューターやアルゴリズムでは、音声のトランスクリプションが発話時間の約半分の時間で実行されるため、2 秒間の発話なら約 1 秒で文字起こしできますが、ユーザーの処理に 1 秒の遅延が発生するアプリケーションでは、これは効率的でもスムーズでもありません。

ただし、ユーザーが発話している間にその発話の別の部分を処理することで、トランスクリプションの時間を "隠す" 方法はあります。 たとえば、1 秒間の発話を 100 ミリ秒のチャンク 10 個に分割し、各チャンクを順番に処理していくことで、合計 500 ミリ秒のトランスクリプション時間のうち、450 ミリ秒以上を "隠し"、発話しているユーザーがトランスクリプションの時間を感じないようにすることができます。 つまり、ユーザーが次の 100 ミリ秒を話している間に、サービスは前の 100 ミリ秒を処理しているということです。したがって、ユーザーが発話を終えたとき、サービスは約 100 ミリ秒の音声を処理するだけで、結果を生成することができます。

このユーザー エクスペリエンスを実現するために、発話された音声情報はチャンクで収集され、ユーザーの発話中に文字起こしされます。 これらの音声は*音声ストリーム*から集合的にチャンク化され、それらの音声チャンクをサービスに送信するプロセスは、*音声ストリーミング*と呼ばれます。 音声ストリーミングは、音声対応のアプリケーションにとって非常に重要な要素となります。チャンク サイズの調整やストリーミング実装の最適化は、アプリケーションのユーザー エクスペリエンス向上に特に大きく影響します。

### <a name="microphones"></a>マイク

人間は耳を使って音声を処理しますが、機械であるハードウェアは、マイクを使ってこれを処理します。 アプリケーションで音声認識を使用できるようにするには、マイクを統合して、アプリケーションに音声ストリームを提供する必要があります。

マイク用の API は、マイクからの音声バイトの受信を開始したり停止したりできるものである必要があります。 開発者は、どのようなユーザー操作が行われたときに、マイクが発話のリッスンを開始するのかを決定する必要があります。 ボタンを押すとリッスンが開始されるようにすることもできますし、*キーワード*や*ウェイク ワード*のスポッターを設定してマイクを常時リッスンし、そのモジュールの出力を使用して Microsoft Speech Service への音声送信をトリガーすることもできます。

### <a name="end-of-speech"></a>発話の終了

話者の発話が*いつ**終わった*のかを検出することは、人間にとっては簡単なことに思えますが、実際の使用環境ではかなり難しい処理になります。 周囲の雑音があることも多いので、発話時間の後の無音状態を単に検知するだけでは不十分です。 Microsoft Speech Service では、ユーザーが発話をいつ終えたのかを高度な処理によってすばやく検出し、発話の終了をアプリケーションに通知することができますが、その仕組み上、アプリケーションはユーザーの発話の終了を最後に知ることになります。 これは、ユーザーの入力の開始*と*終了をアプリケーションが*最初に*知る、他の入力形式とはまったく異なります。

### <a name="asynchronous-service-responses"></a>非同期のサービス応答

ユーザー入力の完了をアプリケーションに知らせる必要があるからといって、アプリケーションのパフォーマンスが低下したり、プログラミングが困難になることはありませんが、音声認識要求については、通常の入力要求や応答パターンとは異なる方法で考えることが必要になります。 ユーザーの発話の停止をアプリケーション側では検知できないため、アプリケーションではサービスからの応答を待機しながら、サービスへの音声ストリーミングを同時かつ非同期的に継続する必要があります。 このパターンは、要求/応答に関する他の Web プロトコル (HTTP など) とは異なります。 通常のプロトコルでは、要求が完了するまで応答は一切受信されませんが、Microsoft Speech Service プロトコルでは、要求の*音声をストリーミングしている最中に*応答を受信します。

> [!NOTE]
> この機能は、Speech HTTP REST API を使用している場合にはサポートされません。

### <a name="turns"></a>ターン

音声は情報の伝達手段です。 人が話すということは、その人が持っている情報を聞き手に伝達しようとしているということです。 通常、人が情報を伝達する際には、話すことと聞くことを順番に行います。 同様に、音声対応のアプリケーションでは、リッスンと応答を交互に行うことでユーザーとの対話を行います。ただし、アプリケーション側では通常、リッスンが主な動作となります。 ユーザーが発話した入力と、その入力に対するサービスの応答は、*ターン*と呼ばれます。 *ターン*は、ユーザーが発話したときに開始され、アプリケーションが音声認識サービスの応答を処理し終えたときに終了します。

### <a name="telemetry"></a>テレメトリ

音声対応のデバイスやアプリケーションを作成することは、経験豊富な開発者であっても困難な場合があります。 ストリーム ベースのプロトコルは、一見すると取り扱いが非常にやっかいに見えることがありますし、無音状態の検出などといった重要な処理も、初めて使用する開発者が少なくありません。 1 つの要求/応答ペアを完了するのにも、多数のメッセージを正常に送受信する必要があるため、それらのメッセージに関するデータを完全かつ正確に収集することが*非常に*重要になります。 Microsoft Speech Service プロトコルは、これらのデータを収集するための手段を提供します。 開発者は、必要なデータをできるかぎり正確に提供するべく、あらゆる手段を尽くす必要があります。完全で正確なデータを提供することが、アプリケーションの性能向上につながります。クライアント実装のトラブルシューティングについて Microsoft Speech Service チームからの支援を要請する際には、収集したテレメトリ データの品質が、問題を分析するうえできわめて重要になります。

> [!NOTE]
> この機能は、音声認識 REST API を使用している場合にはサポートされません。

### <a name="speech-application-states"></a>音声認識アプリケーションの状態

アプリケーションで音声入力を有効にするための手順は、他の入力形式 (マウス クリックやフィンガー タップなど) の手順とは少々異なります。 アプリケーションがいつマイクをリッスンしているかや、音声認識サービスにデータをいつ送信しているか、また、サービスからの応答をいつ待機しているかや、いつアイドル状態になっているかをトラッキングする必要があります。 次の図は、これらの状態間の関係を示したものです。

![音声認識アプリケーションの状態の説明図](Images/speech-application-state-diagram.png)

Microsoft Speech Service はこれらの状態の一部に関わっているため、サービス プロトコルでは、アプリケーションが状態間を移行できるようにするためのメッセージが定義されています。 アプリケーションは、それらプロトコル メッセージを解釈し、対応する動作を実行して、音声アプリケーションの状態をトラッキングし、管理する必要があります。

## <a name="using-the-speech-recognition-service-from-your-apps"></a>アプリから音声認識サービスを利用する

Microsoft 音声認識サービスでは、開発者が音声認識機能をアプリに追加するための手段として、2 つの方法が提供されています。

- [REST API](GetStarted/GetStartedREST.md): アプリからサービスへの HTTP 呼び出しを使用して音声認識を行います。
- [クライアント ライブラリ](GetStarted/GetStartedClientLibraries.md): 高度な機能を使用する場合は、Microsoft Speech クライアント ライブラリをダウンロードして、それらをアプリにリンクできます。  クライアント ライブラリは、各種言語 (C#、Java、JavaScript、ObjectiveC) を使用して、さまざまなプラットフォーム (Windows、Android、iOS) で使用できます。

| ユース ケース | [REST API](GetStarted/GetStartedREST.md) | [クライアント ライブラリ](GetStarted/GetStartedClientLibraries.md) |
|-----|-----|-----|
| 短い発話音声を変換する。たとえば、中間結果を使用しないコマンド (15 秒以下の音声) など | [はい] | [はい] |
| 長い音声 (15 秒以上) を変換する | いいえ  | [はい] |
| 中間結果を使って音声をストリーミングする | いいえ  | [はい] |
| LUIS を使用して、音声から変換されたテキストの意味を解釈する | いいえ  | [はい] |

 言語やプラットフォームに SDK がまだない場合は、[プロトコルのドキュメント](API-Reference-REST/websocketprotocol.md)に基づいて、独自の実装を作成することもできます。

## <a name="recognition-modes"></a>認識モード

認識には 3 つのモードがあります。`interactive`、 `conversation`、および `dictation` です。 認識モードとは、ユーザーがどのように発話するかに基づいて、音声認識の方法を調整するものです。 アプリケーションに応じて、適切な認識モードを選択してください。

> [!NOTE]
> 認識モードの動作は、[REST プロトコル](#rest-speech-recognition-api)を使用した場合と [WebSocket プロトコル](#webSocket-speech-recognition-api)を使用した場合とで異なる場合があります。 たとえば、REST API では、conversation モードや dictation モードであっても、継続的な認識はサポートされません。
> [!NOTE]
> これらのモードは、REST または WebSocket プロトコルを直接使用した場合に適用されます。 [クライアント ライブラリ](GetStarted/GetStartedClientLibraries.md)では、異なるパラメーターを使用して認識モードが指定されます。 詳しくは、ご使用のクライアント ライブラリをご覧ください。

Microsoft Speech Service では、いずれの認識モードにおいても、認識結果の文字列が 1 つだけ返されます。 1 つの発話時間につき、15 秒の制限があります。

### <a name="interactive-mode"></a>対話モード

`interactive` モードでは、ユーザーが短い要求を発し、その応答としてアプリケーションが動作を実行します。

interactive モードのアプリケーションには、一般的に次の特性があります。

- ユーザーが、人に話しているのではなく、コンピューターに話しているということを認識している。
- アプリケーション ユーザーが、アプリケーションに何をして欲しいのかに基づいて、これから言うことを事前に決めている。
- 発話時間は通常 2 ～ 3 秒である。

### <a name="conversation-mode"></a>会話モード

`conversation` モードでは、人間どうしの会話に対して音声認識を行います。

conversation モードのアプリケーションには、一般的に次の特性があります。

- ユーザーが、別の個人に話しているということを認識している。
- 音声認識は、一方または両方の話者に読み上げテキストを表示することで、人間どうし会話を支援する。
- ユーザーは、今から言おうとしていることを必ずしも準備していない。
- ユーザーが、俗語やその他のくだけた表現を頻繁に使用する。

### <a name="dictation-mode"></a>ディクテーション モード

`dictation` モードでは、ユーザーがアプリケーションに対して比較的長い発話を行い、処理を要求します。

dictation モードのアプリケーションには、一般的に次の特性があります。

- ユーザーが、コンピューターに話しているということを認識している。
- ユーザーに、音声認識結果のテキストが表示される。
- 多くの場合、ユーザーはこれから言うことが準備できていて、比較的フォーマルな表現を使用する。
- ユーザーは、全体で 5 ～ 8 秒の文章を発話する。

> [!NOTE]
> dictation モードと conversation モードでは、Microsoft Speech Service は部分結果を返しません。 代わりに、音声ストリーム内の無音状態の区切れ目に、比較的確実な結果文字列が返されます。 Microsoft では今後、これらの継続的な認識モードでのユーザー エクスペリエンスを向上させるために、音声プロトコルを機能強化する可能性があります。

## <a name="recognition-languages"></a>認識言語

*認識言語*は、アプリケーションのユーザーが話す言語を指定するものです。 *認識言語*は、接続の *language* URL クエリ パラメーターを使って指定します。 *language* クエリ パラメーターの値には、IETF 言語タグ [BCP 47](https://en.wikipedia.org/wiki/IETF_language_tag) が使用され、その値は、音声認識 API によってサポートされているいずれかの言語である**必要があります**。 Speech Service でサポートされる言語の一覧は、「[Supported languages (サポートされる言語)](API-Reference-REST/supportedlanguages.md)」で確認できます。

接続要求が無効な場合、Microsoft Speech Service は `HTTP 400 Bad Request` 応答を表示して要求を拒否します。 以下に該当する要求は無効となります。

- *language* クエリ パラメーターの値が含まれていない。
- *language* クエリ パラメーターの形式が正しくない。
- *language* クエリ パラメーターの値がサポート言語ではない。

開発者は、サービスでサポートされている 1 つ以上の言語をサポートしたアプリケーションを作成できます。

### <a name="example"></a>例

次の例では、アプリケーションの音声認識モードが *conversation* に、言語が英語 (米国) に設定されています。

```HTTP
https://speech.platform.bing.com/speech/recognition/conversation/cognitiveservices/v1?language=en-US
```

## <a name="transcription-responses"></a>トランスクリプション応答

トランスクリプション応答では、音声から変換されたテキストがクライアントに返されます。 トランスクリプション応答には、次のフィールドが含まれます。

- `RecognitionStatus`: 認識の状態を示します。 使用される値は、次の表のとおりです。

| 状態 | 説明 |
| ------------- | ---------------- |
| 成功 | 認識が成功し、DisplayText フィールドが存在する |
| NoMatch | 音声ストリーム内に音声が検出されたが、ターゲット言語の単語は見つからなかった。 詳しくは、「NoMatch 認識状態」(#nomatch-recognition-status) をご覧ください  |
| InitialSilenceTimeout | 音声ストリームの先頭に無音状態しか含まれておらず、音声の待機中にサービスがタイムアウトした |
| BabbleTimeout | 音声ストリームの先頭に雑音しか含まれておらず、音声の待機中にサービスがタイムアウトした |
| エラー | 認識サービスで内部エラーが発生し、処理を継続できない |

- `DisplayText` は、大文字/小文字、句読点、および逆テキスト正規化が適用され、不適切表現がアスタリスクでマスクされた後の、認識済み文字列を表します。 DisplayText フィールドは、`RecognitionStatus` フィールドの値が `Success` である場合に*のみ*返されます。

- `Offset` は、語句が認識されたオフセット (100 ナノ秒単位) を示します。このオフセットは、音声ストリームの先頭からの相対位置で示されます。

- `Duration` は、その音声フレーズの持続時間 (100 ナノ秒単位) 示します。

トランスクリプション応答では、必要に応じてさらなる詳細情報を返すこともできます。 詳細な出力を返す方法については、「[出力形式](#output-format)」をご覧ください。

Microsoft Speech Service では、一般的な形式に大文字/小文字、句読点、不適切表現マスク、およびテキスト正規化を追加する、追加のトランスクリプション プロセスがサポートされています。 たとえば、ユーザーが "remind me to buy six iPhones" と発話した場合、Microsoft の Speech Services はトランスクリプション後のテキストとして、"Remind me to buy 6 iPhones" というテキストを返します。 "six" という単語を数字の "6" に変換するプロセスは、*逆テキスト正規化* (略称: *ITN*) と呼ばれます。

### <a name="nomatch-recognition-status"></a>NoMatch 認識状態

Microsoft Speech Service によって音声ストリーム内に音声が検出されたものの、要求に使用されている言語の文法にその音声が一致しなかった場合、トランスクリプション応答では `RecognitionStatus` として `NoMatch` が返されます。 たとえば、認識エンジンが使用言語として英語 (米国) を受け付けている場合に、ユーザーがドイツ語で発話した場合は、*NoMatch* が返される可能性があります。 その場合、発話の波形パターンからは人間による発話が検知されますが、話されている単語のいずれも、認識エンジンで使用されている米国英語の辞書には見つかりません。

また、音声ストリームに含まれている音声に完全に一致する語句を認識アルゴリズムが見つけられなかった場合にも、*NoMatch* が返されます。 その場合、Microsoft Speech Service は*仮定のテキスト*を含んだ *speech.hypothesis* メッセージを生成しつつも、*speech.phrase* メッセージの *RecognitionStatus* は *NoMatch* とする可能性があります。 これは正常な動作です。*speech.hypothesis* メッセージ内のテキストの正確性や不適切表現の有無については、推測で判断しないようにしてください。 また、Microsoft Speech Service が *speech.hypothesis* メッセージを生成したからといって、*speech.phrase* メッセージの *RecognitionStatus* が *Success* になると決めつけないようにしてください。

## <a name="output-format"></a>出力形式

Microsoft Speech Service では、トランスクリプション応答でさまざまなペイロード形式を返すことができます。 すべてのペイロードは JSON 構造になります。

結果文字列の形式は、`format` URL クエリ パラメーターを指定することで制御できます。 既定では、`simple` の結果が返されます。

| 形式 | 説明 |
|-----|-----|
| `simple` | 認識状態と表示形式での認識済みテキストを含んだ、簡略化された結果文字列を返します。 |
| `detailed` | 認識状態と結果文字列の N-best 一覧です。それぞれの結果文字列には、4 つ認識形式と信頼度スコアが含まれます。 |

`detailed` 形式の応答には、`RecognitionStatus`、 `Offset`、および `duration` に加えて、[N-best 値](#n-best-values)が含まれます。

### <a name="n-best-values"></a>N-best 値

人間どうしの会話でもそうですが、リスナー (聞き手) は、発話された内容を*完全に*聞き取れたかどうか知ることができません。 ただしリスナーは、発話内容の特定の解釈に対し、それが正確である*可能性*を割り当てることができます。 

人は通常、頻繁にやりとりする相手と話をするときには、話された言葉を高確率で認識することができます。 コンピューター ベースの音声リスナーも、それと同様の精度を達成するべく設計されており、適切な条件下であれば、[人間と同等の聞き取り精度を実現できます](https://blogs.microsoft.com/next/2016/10/18/historic-achievement-microsoft-researchers-reach-human-parity-conversational-speech-recognition/#sm.001ykosqs14zte8qyxj2k9o28oz5v)。

音声認識で使用されるアルゴリズムは、通常の処理の一部として、発話内容を複数の解釈で捉えます。 通常、これらの解釈は、1 つの解釈の信頼度が高まるにつれて破棄されていきます。 ただし、最適でない条件下では、最終的に複数の解釈の一覧が返されます。 この一覧のうち、上位 *N* 個の解釈が、 *N-best 一覧*と呼ばれます。 それぞれの解釈には、[信頼度スコア](#confidence)が割り当てられます。 信頼度スコアは、0 ～ 1 の範囲で割り当てられます。 スコア 1 は、最高レベルの信頼度を表します。 スコア 0 は、最低レベルの信頼度を表します。

> [!NOTE]
> N-best 一覧内のエントリの数は、発話ごとに異なります。 またエントリ数は、*同じ*発話に対する複数の認識間で異なる場合があります。 このばらつきは、音声認識アルゴリズムの確率論的な性質上、自然なものであり、予期される結果です。

N-best 一覧で返される各エントリには、次の項目が含まれます

- `Confidence`: そのエントリの[信頼度スコア](#confidence)を表します。
- `Lexical`: 認識されたテキストを[語彙形式](#lexical-form)で表したものです。
- `ITN`: 認識されたテキストを [ITN 形式](#itn-form)で表したものです。
- `MaskedITN`: 認識されたテキストを[マスク済み ITN 形式](#masked-itn-form)で表したものです。
- `Display`: 認識されたテキストを[表示形式](#display-form)で表したものです。

### 信頼度スコア <a id="confidence"></a>

信頼度スコアは、音声認識システムにとって不可欠なものです。 Microsoft Speech Service は、*信頼度分類エンジン*から信頼度スコアを取得します。 Microsoft では、正しい解釈と誤った解釈を最大限に区別するよう設計された一連の機能を使って、信頼度分類エンジンをトレーニングしています。 信頼度スコアは、個々の単語と発話全体に対して評価されます。

サービスによって返された信頼度スコアを使用する場合は、次の動作に注意してください。

- 信頼度スコアは、同一の認識モードと言語内でのみ比較できます。 異なる言語や異なる認識モード間でスコアを比較することはしないでください。 たとえば、interactive 認識モードでの信頼度スコアと、dictation モードでの信頼度スコアとの間に相関関係は*ありません*。
- 信頼度スコアは、限定的な発話セットに対して使用するのに適しています。 大規模な発話セットに対するスコアは、必然的に変動の度合いが大きくなります。

信頼度スコアの値をアプリケーションの動作の*しきい値*として使用する場合は、音声認識を使用してしきい値の値を確立してください。

- アプリケーションで使用される発話の代表的なサンプルに対して、音声認識を実行します。
- 各サンプル セット内の認識結果について、信頼度スコアを収集します。
- そのサンプルに対する信頼度の一定のパーセンタイルに基づいて、しきい値を決定します。

すべてのアプリケーションに対して 1 つのしきい値を適用することは適切ではありません。 1 つのアプリケーションで許容される信頼度スコアが、別のアプリケーションでは許容されない場合もあります。

### <a name="lexical-form"></a>語彙形式

語彙形式とは、大文字/小文字や句読点を適用せず、発話の内容を厳密に表した認識済みテキストのことです。 たとえば、"1020 Enterprise Way" という住所の語彙形式は、*ten twenty enterprise way* となります (そのように発話されたと仮定した場合)。 "Remind me to buy 5 pencils" という文の語彙形式は、*remind me to buy five pencils* となります。

語彙形式は、非標準のテキスト正規化を実行する必要があるアプリケーションに最も適しています。 語彙形式はまた、未処理の認識語句を必要とするアプリケーションにも適しています。

語彙形式では、不適切表現はマスクされません。

### <a name="itn-form"></a>ITN 形式

テキスト正規化とは、ある形式のテキストを、別の "正規" 形式のテキストへと変換するプロセスのことです。 たとえば、"555-1212" という電話番号は、*five five five one two one two* という正規形式に変換できます。 *逆*テキスト正規化 (ITN) は、このプロセスを逆方向に処理するものです。つまり、"five five five one two one two" という語句を、*555-1212* という逆正規形式に変換します。 ITN 形式の認識結果には、大文字/小文字や句読点は含まれません。

ITN 形式は、認識済みテキストに対して動作を実行するアプリケーションに最も適しています。 たとえば、ユーザーが検索語を発話した後、それらの語句を Web 検索に使用するアプリケーションでは、ITN 検索が使用されます。 ITN 形式では、不適切表現はマスクされません。 不適切表現をマスクするには、*マスク済み ITN 形式*を使用します。

### <a name="masked-itn-form"></a>マスク済み ITN 形式

不適切表現は話し言葉に自然と含まれてくるものなので、Microsoft Speech Service では、発話に含まれるそれらの語句も認識されるようになっています。 ただし、不適切表現はすべてのアプリケーションで使用できるものではありません。特に、ユーザー制限のあるアプリケーションや未成年向けのアプリケーションにおいては、使用できない可能性があります。

マスク済み ITN 形式とは、逆テキスト正規化形式に不適切表現のマスキングを適用するものです。 不適切表現をマスクするには、不適切表現パラメーターの値を `masked` に設定します。 不適切表現をマスクした場合、言語の不適切表現辞書に一致項目が見つかった語句は、アスタリスクに置き換えられます。 たとえば、*remind me to buy 5 **** pencils* のようになります。 マスク済み ITN 形式の認識結果には、大文字/小文字や句読点は含まれません。

> [!NOTE]
> profanity クエリ パラメーターの値が `raw` に設定された場合、マスク済み ITN 形式は、ITN 形式と同じになります。 不適切表現はマスク*されません*。

### <a name="display-form"></a>表示形式

句読点や大文字/小文字を使用すると、強調される箇所や文章の区切れ目が示され、テキストがわかりやすくなります。 表示形式は、認識結果に句読点や大文字/小文字を追加する形式です。この形式は、発話されたテキストを表示するアプリケーションに最も適しています。

表示形式はマスク済み ITN 形式を拡張したものなので、不適切表現パラメーターの値を `masked` または `raw` に設定することができます。 この値が `raw` に設定された場合、認識結果には、ユーザーが発話した不適切表現がすべて含まれます。 値が `masked` に設定された場合、言語の不適切表現辞書に一致項目が見つかった語句は、アスタリスクに置き換えられます。

### <a name="sample-responses"></a>応答のサンプル

すべてのペイロードは JSON 構造になります。

`simple` の結果文字列のペイロード形式はつぎのようになります。

```json
{
  "RecognitionStatus": "Success",
  "DisplayText": "Remind me to buy 5 pencils.",
  "Offset": "1236645672289",
  "Duration": "1236645672289"
}
```

`detailed` の結果文字列のペイロード形式はつぎのようになります。

```json
{
  "RecognitionStatus": "Success",
  "Offset": "1236645672289",
  "Duration": "1236645672289",
  "NBest": [
      {
        "Confidence" : "0.87",
        "Lexical" : "remind me to buy five pencils",
        "ITN" : "remind me to buy 5 pencils",
        "MaskedITN" : "remind me to buy 5 pencils",
        "Display" : "Remind me to buy 5 pencils.",
      },
      {
        "Confidence" : "0.54",
        "Lexical" : "rewind me to buy five pencils",
        "ITN" : "rewind me to buy 5 pencils",
        "MaskedITN" : "rewind me to buy 5 pencils",
        "Display" : "Rewind me to buy 5 pencils.",
      }
  ]
}
```

## <a name="profanity-handling-in-speech-recognition"></a>音声認識での不適切表現の処理

Microsoft Speech Service では、多くの人が "不適切" として分類する語句を含め、人が発するすべての形式の音声が認識されます。 このサービスにおける不適切表現の処理方法は、*profanity* クエリ パラメーターを使用して制御できます。 既定では、*speech.phrase* 結果内の不適切表現はマスクされ、不適切表現が含まれる *speech.hypothesis* メッセージは返されません。

| *profanity* の値 | 説明 |
| - | - |
| `masked` | 不適切表現をアスタリスクでマスクします。 これが既定の動作となります。 | 
| `removed` | すべての結果から不適切表現を削除します。 |
| `raw` | すべての結果で不適切表現を認識し、それらを返します。 |

### <a name="profanity-value-masked"></a>Profanity 値 `Masked`

不適切表現をマスクするには、*profanity* クエリ パラメーターの値を *masked* に設定します。 要求で *profanity* クエリ パラメーターがこの値に設定されているか、指定されていない場合は、不適切表現が*マスク*されます。 本サービスでは、認識された結果に含まれる不適切表現をアスタリスクに置き換えることで、マスキングが実行されます。 不適切表現のマスキング処理を指定した場合、不適切表現が含まれている *speech.hypothesis* メッセージは返されません。

### <a name="profanity-value-removed"></a>Profanity 値 `Removed`

*profanity* クエリ パラメーターの値が *removed* に設定されている場合は、*speech.phrase* メッセージと *speech.hypothesis* メッセージの両方から不適切表現が削除されます。 結果は、*不適切な語句が発話されなかった場合*と同様の内容になります。

#### <a name="profanity-only-utterances"></a>不適切表現のみの発話

場合によっては、アプリケーションが不適切表現を削除するように構成されている場合に、ユーザーが不適切表現*だけ*を発話する可能性もあります。 そのような場合、認識モードが *dictation* または *conversation* になっていると、*speech.result* は返されません。 認識モードが *interactive* の場合は、状態コードが *NoMatch* になった状態で *speech.result* が返されます。 

### <a name="profanity-value-raw"></a>Profanity 値 `Raw`

*profanity* クエリ パラメーターの値が *raw* に設定されている場合は、*speech.phrase* メッセージと *speech.hypothesis* メッセージのいずれについても、不適切表現の削除やマスキングは行われません。
